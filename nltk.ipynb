{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Toolkit(NLTK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Steps in NLP:\n",
    "#- Data Collection\n",
    "#- Tokenization, Stopwards Removal, Stemming\n",
    "#- Building a common vocab\n",
    "#- Vectorize the documents\n",
    "#- Performing Classification/Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NlTK deals with the data in the form of text or words with the help of corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Corpora means a collection of written texts, especially the entire works of a particular author or a body of writing on a particular subject.\n",
    "# The collection of such data(corpora) is known as corpus\n",
    "# corpus is like the training dataset in text form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\lavanya\n",
      "[nltk_data]     rajeswari\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "#nltk.download()-to view all the corpus available in the nltk\n",
    "nltk.download('punkt')#punkt is one of the corpus package name "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to C:\\Users\\lavanya\n",
      "[nltk_data]     rajeswari\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('brown')\n",
    "from nltk.corpus import brown#brown corpus is the traing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['adventure', 'belles_lettres', 'editorial', 'fiction', 'government', 'hobbies', 'humor', 'learned', 'lore', 'mystery', 'news', 'religion', 'reviews', 'romance', 'science_fiction']\n"
     ]
    }
   ],
   "source": [
    "print(brown.categories())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'nltk.collections.LazySubsequence'>\n",
      "[['Assembly', 'session', 'brought', 'much', 'good'], ['The', 'General', 'Assembly', ',', 'which', 'adjourns', 'today', ',', 'has', 'performed', 'in', 'an', 'atmosphere', 'of', 'crisis', 'and', 'struggle', 'from', 'the', 'day', 'it', 'convened', '.'], ...]\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "data = brown.sents(categories='editorial')[:100]#returns sentences of the given category\n",
    "#prints the 1st 100 sentences\n",
    "print(type(data))\n",
    "print(data)\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization and Stopword removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#random text\n",
    "text = \"It was a very pleasant day, the weather was cool and there were showers. I went to market to buy some fruits.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenization means splitting the sentence into set of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sent_tokenize splits the data into sentences\n",
    "#word_tokenize splits the data into words\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['It was a very pleasant day, the weather was cool and there were showers.',\n",
       " 'I went to market to buy some fruits.']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents = sent_tokenize(text)#tokenizes sentences based on fullstop(.) punctuation mark\n",
    "sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = word_tokenize(sents[0].lower())#word tokenization of 1st sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['it',\n",
       " 'was',\n",
       " 'a',\n",
       " 'very',\n",
       " 'pleasant',\n",
       " 'day',\n",
       " ',',\n",
       " 'the',\n",
       " 'weather',\n",
       " 'was',\n",
       " 'cool',\n",
       " 'and',\n",
       " 'there',\n",
       " 'were',\n",
       " 'showers',\n",
       " '.']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stopword Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\lavanya\n",
      "[nltk_data]     rajeswari\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Stopwords means which doesnt contribute much in the text like if,of,the etc...\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw = set(stopwords.words('english'))#displays stopwords in english language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'above', 'are', \"it's\", 'will', \"hasn't\", 'each', 'than', 'it', 'until', 't', \"you'll\", \"didn't\", 'does', 'mustn', 'before', 'doing', 'hers', 'is', 'up', 'then', 'shan', 'against', 'my', 'hasn', 'should', 'your', \"wasn't\", 'he', 'i', 'himself', 'down', 'by', 'own', 'but', 'only', 'between', 've', 'to', 'other', \"shouldn't\", \"wouldn't\", \"aren't\", 'which', 'being', 'why', 'any', 'once', 'most', 'its', 'not', 'those', 'm', 'their', 'whom', 'weren', 'couldn', 'our', 'haven', 'mightn', 'very', 'such', \"doesn't\", 'under', 'herself', 'below', 'aren', 'at', 'no', 'her', \"hadn't\", 'were', 'as', 'this', 'd', 'because', 'from', 'and', 'has', 'so', 'both', 'wasn', 'yourself', 'isn', 'out', 'that', \"couldn't\", 'didn', 'ma', \"weren't\", 'more', 'there', 'further', 'in', 'an', 'into', 'wouldn', 'nor', 'ourselves', \"she's\", 'where', 'have', 'his', 'all', \"needn't\", 'y', \"haven't\", 'we', 'll', 'do', 'or', 'been', 'a', 'am', 'while', 'yourselves', \"shan't\", 'shouldn', 'be', 'of', \"you've\", 'theirs', 'few', 'what', 'how', 'don', 'him', 'same', \"mightn't\", 'when', 'them', 'themselves', 'through', \"you'd\", 'too', \"mustn't\", 'ain', 'they', \"that'll\", 'if', 'about', 'during', \"you're\", 'off', 'needn', \"won't\", 'again', 're', 'some', 'ours', 'can', 'o', 'the', \"should've\", 'doesn', 'having', 'now', 'myself', 'who', 'had', 'hadn', 'did', 'itself', 'for', \"don't\", 'after', 'me', 's', 'won', 'was', 'just', 'with', 'yours', 'over', 'here', 'these', \"isn't\", 'on', 'she', 'you'} 179\n"
     ]
    }
   ],
   "source": [
    "print(sw,len(sw))#179 stopwords are there in-built"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter the words from the sentence, i.e., removing the stopwords from the sentence\n",
    "def filter_words(word_list):\n",
    "    useful_words = [w for w in word_list if w not in sw]\n",
    "    return useful_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pleasant', 'day', ',', 'weather', 'cool', 'showers', '.']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "useful_words = filter_words(word_list)\n",
    "useful_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RegexpTokenizer tokenizes the text based on regular expressions\n",
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(\"[a-zA-Z0-9]+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['send', 'the', '50', 'documents', 'to', 'abc', 'def', 'ghi']\n"
     ]
    }
   ],
   "source": [
    "sents = \"send the 50 documents to abc, def, ghi.\"\n",
    "print(tokenizer.tokenize(sents))#gives the words which are lowercase or uppercase or digits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming\n",
    "- stemming means transforms the verb or any other word to root word\n",
    "- Ex: jumping,jumps,jump is transformed into jump etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Types of Stemmers\n",
    "- Snowball Stemmer (Multilingual)\n",
    "- PorterStemmer, LancasterStemmer(English language only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing different types of stemmer\n",
    "from nltk.stem.snowball import PorterStemmer, SnowballStemmer\n",
    "from nltk.stem.lancaster import LancasterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PorterStemmer, LancasterStemmer uses different algorithms for Stemming\n",
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'jump'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps.stem(\"jumped\")\n",
    "ps.stem(\"jumping\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'love'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps.stem(\"lovely\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "awesom\n",
      "awesom\n",
      "teen\n",
      "teenag\n"
     ]
    }
   ],
   "source": [
    "print(ps.stem(\"awesome\"))\n",
    "ls = LancasterStemmer()\n",
    "print(ls.stem(\"awesome\"))\n",
    "\n",
    "print(ls.stem(\"teenager\"))\n",
    "print(ps.stem(\"teenager\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'buan'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ss = SnowballStemmer('spanish')\n",
    "ss.stem('buano')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Bag of Words approach is storing the frequency of the corresponding words and the words index also\n",
    "- Bag of words are collected by removing the stop words, text filtering etc.. from the given corpus\n",
    "- each word in a bag of word represents a column\n",
    "- It uses one hot encoding process means, when the word is found, 1 is placed in the column(word vector)\n",
    "- the similarity between the word vectors is found by pairwise euclidean distance between the rows\n",
    "- k-products(rows) which have closer distance are the required recommended products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    'Indian team will win today',\n",
    "    'Lockdown expected to end by May 2020',\n",
    "    'Colleges and schools are closed due to coronavirus pandemic',\n",
    "    'There is nothing to talk about other than corona virus'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Indian team will win today', 'Lockdown expected to end by May 2020', 'Colleges and schools are closed due to coronavirus pandemic', 'There is nothing to talk about other than corona virus']\n"
     ]
    }
   ],
   "source": [
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to convert words into numerical features\n",
    "# Building a common vocabulary and vectorize the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "def myTokenizer(sentence):\n",
    "    words = tokenizer.tokenize(sentence.lower())\n",
    "    return filter_words(words)\n",
    "\n",
    "list_words = myTokenizer(corpus[0])\n",
    "print(len(list_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizer parameter takes the value as the function created by user for tokenization of words\n",
    "#mytokenizerfunction is used for the stopword removal, text filtering\n",
    "#ngram feature is used to take words upto a range(Change the range in function for better understanding)\n",
    "cv = CountVectorizer(tokenizer = myTokenizer,ngram_range = (1,1) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gives the sparse matrix of the word of vectors\n",
    "vectorized_corpus = cv.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting the result to array(sparse matrxix as most 0's will be present in the result) for reducing space complexity\n",
    "vc = vectorized_corpus.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 1 0 1]\n",
      " [1 0 0 0 0 0 1 1 0 1 1 0 0 0 0 0 0 0 0]\n",
      " [0 1 1 0 1 1 0 0 0 0 0 0 1 1 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0 0 0 0 0 1 0 0 1 0 0 1 0]]\n",
      "{'indian': 8, 'team': 15, 'win': 18, 'today': 16, 'lockdown': 9, 'expected': 7, 'end': 6, 'may': 10, '2020': 0, 'colleges': 2, 'schools': 13, 'closed': 1, 'due': 5, 'coronavirus': 4, 'pandemic': 12, 'nothing': 11, 'talk': 14, 'corona': 3, 'virus': 17}\n"
     ]
    }
   ],
   "source": [
    "print(vc)\n",
    "print(cv.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(['indian', 'team', 'today', 'win'], dtype='<U11'),\n",
       " array(['2020', 'end', 'expected', 'lockdown', 'may'], dtype='<U11'),\n",
       " array(['closed', 'colleges', 'coronavirus', 'due', 'pandemic', 'schools'],\n",
       "       dtype='<U11'),\n",
       " array(['corona', 'nothing', 'talk', 'virus'], dtype='<U11')]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#gives the sentences with words contained in the bag of words\n",
    "cv.inverse_transform(vc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words drawbacks:\n",
    "- it doesnt maintain any order of words that appear in sentences\n",
    "- we cant get the context from two words which have same meaning(Ex:worldcup, worldcups)\n",
    "- takes huge memory (ex: if there are 10k words, 1st sentence contains only 3 words then remaining memory is wasted for storing 0 in other words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- It gives good result than BOW approach\n",
    "- But it also fails in the context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- TF means term Frequency, IDF means Inverse Document Frequency\n",
    "- TF for a word in a document means number of times word occurs in document(count vectorizer approach)\n",
    "- IDF for a word in the entire corpus is computed by the formula:\n",
    "   log(no of documents we have/no of documents(sentences) we have the word in it)\n",
    "- IDF gives the rareness(occurences) of the word in the corpus\n",
    "   Ex:'The world cup held in india','this name is awesome'\n",
    "   IDF for  word(world) in above corpus is, log(2/1)\n",
    "- final weight assigned for each word is product of TF and IDF of a word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(tokenizer = myTokenizer, ngram_range = (1,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In BOW euclidean distance is calculated to describe similarity\n",
    "- In TF-IDF, cosine similairity is used to calculate similarity\n",
    "- cosine similarity means the cosine of angle between the 2 vectors\n",
    "- cosine similarity=dot product between 2 vectors/products of magnitudes of individual vectors\n",
    "- if cosine similarity is nearer to 1 =>angle between them is 0 which means the vectors are very similar "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.37796447 0.37796447 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.37796447 0.37796447\n",
      "  0.37796447 0.         0.37796447 0.37796447]\n",
      " [0.33333333 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.33333333\n",
      "  0.33333333 0.33333333 0.33333333 0.         0.         0.33333333\n",
      "  0.33333333 0.33333333 0.33333333 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.30151134 0.30151134 0.30151134 0.30151134 0.\n",
      "  0.         0.30151134 0.30151134 0.30151134 0.30151134 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.30151134\n",
      "  0.30151134 0.30151134 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.37796447\n",
      "  0.37796447 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.37796447 0.37796447 0.\n",
      "  0.         0.         0.37796447 0.37796447 0.         0.\n",
      "  0.         0.37796447 0.         0.        ]]\n",
      "{'indian': 15, 'team': 28, 'win': 32, 'today': 30, 'indian team': 16, 'team win': 29, 'win today': 33, 'lockdown': 17, 'expected': 13, 'end': 11, 'may': 19, '2020': 0, 'lockdown expected': 18, 'expected end': 14, 'end may': 12, 'may 2020': 20, 'colleges': 3, 'schools': 24, 'closed': 1, 'due': 9, 'coronavirus': 7, 'pandemic': 23, 'colleges schools': 4, 'schools closed': 25, 'closed due': 2, 'due coronavirus': 10, 'coronavirus pandemic': 8, 'nothing': 21, 'talk': 26, 'corona': 5, 'virus': 31, 'nothing talk': 22, 'talk corona': 27, 'corona virus': 6}\n"
     ]
    }
   ],
   "source": [
    "vectorized_corpus = tfidf_vectorizer.fit_transform(corpus).toarray()\n",
    "print(vectorized_corpus)\n",
    "print(tfidf_vectorizer.vocabulary_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
